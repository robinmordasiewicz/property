<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>
	Comments for Aspen Mesh	</title>
	<atom:link href="https://aspenmesh.io/comments/feed/" rel="self" type="application/rss+xml" />
	<link>https://aspenmesh.io</link>
	<description></description>
	<lastBuildDate>Wed, 22 Jul 2020 01:56:23 +0000</lastBuildDate>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.8.2</generator>
	<item>
		<title>
		Comment on Announcing Aspen Mesh Secure Ingress Policy by Jing Lin		</title>
		<link>https://aspenmesh.io/announcing-aspen-mesh-secure-ingress-policy/#comment-2365</link>

		<dc:creator><![CDATA[Jing Lin]]></dc:creator>
		<pubDate>Wed, 22 Jul 2020 01:56:23 +0000</pubDate>
		<guid isPermaLink="false">https://aspenmesh.io/?p=4951#comment-2365</guid>

					<description><![CDATA[Great thinking that match the real production need. Will this integrated with F5 DNS(and/or  F5DNSaaS)?]]></description>
			<content:encoded><![CDATA[<p>Great thinking that match the real production need. Will this integrated with F5 DNS(and/or  F5DNSaaS)?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		Comment on Running Stateful Apps with Service Mesh: Kubernetes Cassandra with Istio mTLS Enabled by Satish Anupindi		</title>
		<link>https://aspenmesh.io/running-stateless-apps-with-service-mesh-kubernetes-cassandra-with-istio-mtls-enabled/#comment-2083</link>

		<dc:creator><![CDATA[Satish Anupindi]]></dc:creator>
		<pubDate>Sun, 28 Jun 2020 20:50:48 +0000</pubDate>
		<guid isPermaLink="false">https://aspenmesh.io/?p=2041#comment-2083</guid>

					<description><![CDATA[Would there be any changes to the way the Cassandra driver connects to the Cluster using the mesh? Will the driver still need to use the FQDN of the Cassandra seed node to establish connection with the cluster]]></description>
			<content:encoded><![CDATA[<p>Would there be any changes to the way the Cassandra driver connects to the Cluster using the mesh? Will the driver still need to use the FQDN of the Cassandra seed node to establish connection with the cluster</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		Comment on Building Istio with Minikube-in-a-Container and Jenkins by Nick		</title>
		<link>https://aspenmesh.io/building-istio-with-minikube-in-a-container-and-jenkins/#comment-841</link>

		<dc:creator><![CDATA[Nick]]></dc:creator>
		<pubDate>Mon, 09 Mar 2020 12:13:41 +0000</pubDate>
		<guid isPermaLink="false">https://aspenmesh.io/?p=129#comment-841</guid>

					<description><![CDATA[Could you run this without privileged, but using rootless?  How would that be done?]]></description>
			<content:encoded><![CDATA[<p>Could you run this without privileged, but using rootless?  How would that be done?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		Comment on Top 3 Service Mesh Developments in 2019 by Michael Quintero		</title>
		<link>https://aspenmesh.io/top-3-service-mesh-developments-in-2019/#comment-174</link>

		<dc:creator><![CDATA[Michael Quintero]]></dc:creator>
		<pubDate>Thu, 26 Dec 2019 13:40:39 +0000</pubDate>
		<guid isPermaLink="false">https://aspenmesh.io/?p=1953#comment-174</guid>

					<description><![CDATA[Thank you to the Aspen Mesh team for offering some insights and guidance on the service mesh landscape.  It&#039;s December 2019 now, and we can see that many of these  guesses were prophetic!
I look very much forward to the &quot;2020&quot; edition... exciting times :)]]></description>
			<content:encoded><![CDATA[<p>Thank you to the Aspen Mesh team for offering some insights and guidance on the service mesh landscape.  It&#8217;s December 2019 now, and we can see that many of these  guesses were prophetic!<br />
I look very much forward to the &#8220;2020&#8221; edition&#8230; exciting times üôÇ</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		Comment on Improving Microservices: Weighing Service Mesh Options and Benefits by Bart Van Bos		</title>
		<link>https://aspenmesh.io/improving-microservices-service-mesh-options-benefits/#comment-17</link>

		<dc:creator><![CDATA[Bart Van Bos]]></dc:creator>
		<pubDate>Thu, 25 Jul 2019 20:44:32 +0000</pubDate>
		<guid isPermaLink="false">https://aspenmesh.io/?p=3674#comment-17</guid>

					<description><![CDATA[Great article, Zach Jory! Fully agree on the 3 main reasons of service mesh adoption: observability, mTLS and distributed tracing. Observability and distributed tracing are a pain in the butt in polyglot microservice environments.]]></description>
			<content:encoded><![CDATA[<p>Great article, Zach Jory! Fully agree on the 3 main reasons of service mesh adoption: observability, mTLS and distributed tracing. Observability and distributed tracing are a pain in the butt in polyglot microservice environments.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		Comment on Running Stateful Apps with Service Mesh: Kubernetes Cassandra with Istio mTLS Enabled by Joe		</title>
		<link>https://aspenmesh.io/running-stateless-apps-with-service-mesh-kubernetes-cassandra-with-istio-mtls-enabled/#comment-16</link>

		<dc:creator><![CDATA[Joe]]></dc:creator>
		<pubDate>Thu, 30 May 2019 17:40:51 +0000</pubDate>
		<guid isPermaLink="false">https://aspenmesh.io/?p=2041#comment-16</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://aspenmesh.io/running-stateless-apps-with-service-mesh-kubernetes-cassandra-with-istio-mtls-enabled/#comment-15&quot;&gt;Andrew Sorensen&lt;/a&gt;.

Hi Andrew, we agree that both of your points are valid, but we have not seen this impact our Cassandra environments.  We would love to know more about your configuration.  Feel free to reach out directly. (support@aspenmesh.io or joe@aspenmsh.io)

Regarding your first point:
By doing a port-forward of a cassandra node and looking at the envoy config_dump, it appears that envoy is assigning the node ips as ‚Äúinbound‚Äù rather than ‚Äúoutbound‚Äù.  This is the crux of issue #12551 and will impact most clustered, stateful apps.

In our environment though, the Cassandra nodes either do not ask for themselves or are smart enough to know who the local node is. The gossip is able to communicate and all three cassandra pods successfully start. (See below)

cassandra-0              2/2     Running   1          91d   100.96.3.67
cassandra-1              2/2     Running   2          91d   100.96.4.74    
cassandra-2              2/2     Running   3          91d   100.96.1.241   

$kubectl -n cassandra port-forward cassandra-0 15000  
Then looking at the config_dump:
.
.
.
&quot;dynamic_active_listeners&quot;: [
     {
             &quot;version_info&quot;: &quot;2019-05-13T23:04:14Z/94&quot;,
&quot;listener&quot;: {
   &quot;name&quot;: &quot;100.96.3.67_9042&quot;,
                 &quot;address&quot;: {
                       &quot;socket_address&quot;: {
                            &quot;address&quot;: &quot;100.96.3.67&quot;,
                            &quot;port_value&quot;: 9042
                         }
.
.
.
                     {
                        &quot;name&quot;: &quot;envoy.tcp_proxy&quot;, 
                        &quot;config&quot;: {
                        &quot;stat_prefix&quot;: &quot;inbound&#124;9042&#124;tcp-client&#124;cassandra.cassandra.svc.cluster.local&quot;,
                        &quot;cluster&quot;: &quot;inbound&#124;9042&#124;tcp-client&#124;cassandra.cassandra.svc.cluster.local&quot;,

Regarding the second point, it is correct that the nodes are identifying with the src IP and envoy passes the connection through as 127.0.0.1, but again this did not seem to be a problem in our environment.

$ kubectl -n cassandra exec -it cassandra-0 nodetool status
Datacenter: DC1-K8Demo
======================
Status=Up/Down
&#124;/ State=Normal/Leaving/Joining/Moving
--  Address       Load       Tokens       Owns (effective)  Host ID                               Rack
UN  100.96.1.241  151.04 KiB  32           55.4%             57679164-f95f-45f2-a0d6-856c62874620  Rack1-K8Demo
UN  100.96.3.67   158.03 KiB  32           72.8%             cc4d56c7-9931-4a9b-8d6a-d7db8c4ea67b  Rack1-K8Demo
UN  100.96.4.74   126.22 KiB  32           71.8%             f65e8c93-85d7-4b8b-ae82-66f26b36d5fd  Rack1-K8Demo

$ kubectl -n cassandra exec -ti cqlsh-5d648594cb-86rq9  bash
# cqlsh 100.96.3.67 9042
Connected to K8Demo at 100.96.3.67:9042.
[cqlsh 5.0.1 &#124; Cassandra 3.11.2 &#124; CQL spec 3.4.4 &#124; Native protocol v4]
Use HELP for help.
cqlsh&#062;
cqlsh&#062; use system_schema;
cqlsh:system_schema&#062; select keyspace_name,table_name from tables where keyspace_name = &#039;system&#039;;

 keyspace_name &#124; table_name
---------------+--------------------------
        system &#124;                IndexInfo
        system &#124;         available_ranges
        system &#124;                  batches
.
.
.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://aspenmesh.io/running-stateless-apps-with-service-mesh-kubernetes-cassandra-with-istio-mtls-enabled/#comment-15">Andrew Sorensen</a>.</p>
<p>Hi Andrew, we agree that both of your points are valid, but we have not seen this impact our Cassandra environments.  We would love to know more about your configuration.  Feel free to reach out directly. (support@aspenmesh.io or <a href="mailto:joe@aspenmsh.io">joe@aspenmsh.io</a>)</p>
<p>Regarding your first point:<br />
By doing a port-forward of a cassandra node and looking at the envoy config_dump, it appears that envoy is assigning the node ips as ‚Äúinbound‚Äù rather than ‚Äúoutbound‚Äù.  This is the crux of issue #12551 and will impact most clustered, stateful apps.</p>
<p>In our environment though, the Cassandra nodes either do not ask for themselves or are smart enough to know who the local node is. The gossip is able to communicate and all three cassandra pods successfully start. (See below)</p>
<p>cassandra-0              2/2     Running   1          91d   100.96.3.67<br />
cassandra-1              2/2     Running   2          91d   100.96.4.74<br />
cassandra-2              2/2     Running   3          91d   100.96.1.241   </p>
<p>$kubectl -n cassandra port-forward cassandra-0 15000<br />
Then looking at the config_dump:<br />
.<br />
.<br />
.<br />
&#8220;dynamic_active_listeners&#8221;: [<br />
     {<br />
             &#8220;version_info&#8221;: &#8220;2019-05-13T23:04:14Z/94&#8221;,<br />
&#8220;listener&#8221;: {<br />
   &#8220;name&#8221;: &#8220;100.96.3.67_9042&#8221;,<br />
                 &#8220;address&#8221;: {<br />
                       &#8220;socket_address&#8221;: {<br />
                            &#8220;address&#8221;: &#8220;100.96.3.67&#8221;,<br />
                            &#8220;port_value&#8221;: 9042<br />
                         }<br />
.<br />
.<br />
.<br />
                     {<br />
                        &#8220;name&#8221;: &#8220;envoy.tcp_proxy&#8221;,<br />
                        &#8220;config&#8221;: {<br />
                        &#8220;stat_prefix&#8221;: &#8220;inbound|9042|tcp-client|cassandra.cassandra.svc.cluster.local&#8221;,<br />
                        &#8220;cluster&#8221;: &#8220;inbound|9042|tcp-client|cassandra.cassandra.svc.cluster.local&#8221;,</p>
<p>Regarding the second point, it is correct that the nodes are identifying with the src IP and envoy passes the connection through as 127.0.0.1, but again this did not seem to be a problem in our environment.</p>
<p>$ kubectl -n cassandra exec -it cassandra-0 nodetool status<br />
Datacenter: DC1-K8Demo<br />
======================<br />
Status=Up/Down<br />
|/ State=Normal/Leaving/Joining/Moving<br />
&#8212;  Address       Load       Tokens       Owns (effective)  Host ID                               Rack<br />
UN  100.96.1.241  151.04 KiB  32           55.4%             57679164-f95f-45f2-a0d6-856c62874620  Rack1-K8Demo<br />
UN  100.96.3.67   158.03 KiB  32           72.8%             cc4d56c7-9931-4a9b-8d6a-d7db8c4ea67b  Rack1-K8Demo<br />
UN  100.96.4.74   126.22 KiB  32           71.8%             f65e8c93-85d7-4b8b-ae82-66f26b36d5fd  Rack1-K8Demo</p>
<p>$ kubectl -n cassandra exec -ti cqlsh-5d648594cb-86rq9  bash<br />
# cqlsh 100.96.3.67 9042<br />
Connected to K8Demo at 100.96.3.67:9042.<br />
[cqlsh 5.0.1 | Cassandra 3.11.2 | CQL spec 3.4.4 | Native protocol v4]<br />
Use HELP for help.<br />
cqlsh&gt;<br />
cqlsh&gt; use system_schema;<br />
cqlsh:system_schema&gt; select keyspace_name,table_name from tables where keyspace_name = &#8216;system&#8217;;</p>
<p> keyspace_name | table_name<br />
&#8212;&#8212;&#8212;&#8212;&#8212;+&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8212;&#8211;<br />
        system |                IndexInfo<br />
        system |         available_ranges<br />
        system |                  batches<br />
.<br />
.<br />
.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		Comment on Running Stateful Apps with Service Mesh: Kubernetes Cassandra with Istio mTLS Enabled by Andrew Sorensen		</title>
		<link>https://aspenmesh.io/running-stateless-apps-with-service-mesh-kubernetes-cassandra-with-istio-mtls-enabled/#comment-15</link>

		<dc:creator><![CDATA[Andrew Sorensen]]></dc:creator>
		<pubDate>Thu, 30 May 2019 05:17:43 +0000</pubDate>
		<guid isPermaLink="false">https://aspenmesh.io/?p=2041#comment-15</guid>

					<description><![CDATA[I&#039;ve been trying to replicate this setup for a small environment, and have been struggling with a few things. I&#039;m not sure if there&#039;s been changes to istio or perhaps some details I missed or were left out. How did you overcome these challenges:

* first node in statefulset can&#039;t talk to itself, since istio doesn&#039;t allow a host to talk to itself? (https://github.com/istio/istio/issues/12551)
* cassandra expects actual src ip address to match, but istio proxies and causes the src ip to be &quot;127.0.01&quot;?]]></description>
			<content:encoded><![CDATA[<p>I&#8217;ve been trying to replicate this setup for a small environment, and have been struggling with a few things. I&#8217;m not sure if there&#8217;s been changes to istio or perhaps some details I missed or were left out. How did you overcome these challenges:</p>
<p>* first node in statefulset can&#8217;t talk to itself, since istio doesn&#8217;t allow a host to talk to itself? (<a href="https://github.com/istio/istio/issues/12551" rel="nofollow ugc">https://github.com/istio/istio/issues/12551</a>)<br />
* cassandra expects actual src ip address to match, but istio proxies and causes the src ip to be &#8220;127.0.01&#8221;?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		Comment on Tracing gRPC with Istio by Neeraj Poddar		</title>
		<link>https://aspenmesh.io/tracing-grpc-with-istio/#comment-12</link>

		<dc:creator><![CDATA[Neeraj Poddar]]></dc:creator>
		<pubDate>Mon, 01 Apr 2019 22:23:19 +0000</pubDate>
		<guid isPermaLink="false">https://aspenmesh.io/?p=152#comment-12</guid>

					<description><![CDATA[Hi Dave,

Thanks for your question. You&#039;re totally correct in mentioning that gRPC connections are long lived and as such you don&#039;t want to re-create outgoing connection for every new incoming request.

Even though my example is not clear, you can insert the client side interceptors while making a new client connection outside the context of the incoming request. In that case you can pass the incoming request context to the outgoing gRPC request call and the client interceptors will still work as expected i.e. propagate tracing headers if present in the incoming request&#039;s context.]]></description>
			<content:encoded><![CDATA[<p>Hi Dave,</p>
<p>Thanks for your question. You&#8217;re totally correct in mentioning that gRPC connections are long lived and as such you don&#8217;t want to re-create outgoing connection for every new incoming request.</p>
<p>Even though my example is not clear, you can insert the client side interceptors while making a new client connection outside the context of the incoming request. In that case you can pass the incoming request context to the outgoing gRPC request call and the client interceptors will still work as expected i.e. propagate tracing headers if present in the incoming request&#8217;s context.</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		Comment on Tracing gRPC with Istio by Dave Newlin		</title>
		<link>https://aspenmesh.io/tracing-grpc-with-istio/#comment-11</link>

		<dc:creator><![CDATA[Dave Newlin]]></dc:creator>
		<pubDate>Wed, 27 Mar 2019 21:21:25 +0000</pubDate>
		<guid isPermaLink="false">https://aspenmesh.io/?p=152#comment-11</guid>

					<description><![CDATA[gRPC connections are by nature long-lived and safe for concurrent use. Your example requires that the gRPC connection be created with the context of the incoming request, meaning that the connection is scoped to that request. 

Is it possible to propagate tracing headers by passing the incoming request context to the gRPC client methods, or is it required to create a new connection for each incoming request?]]></description>
			<content:encoded><![CDATA[<p>gRPC connections are by nature long-lived and safe for concurrent use. Your example requires that the gRPC connection be created with the context of the incoming request, meaning that the connection is scoped to that request. </p>
<p>Is it possible to propagate tracing headers by passing the incoming request context to the gRPC client methods, or is it required to create a new connection for each incoming request?</p>
]]></content:encoded>
		
			</item>
		<item>
		<title>
		Comment on Running Stateful Apps with Service Mesh: Kubernetes Cassandra with Istio mTLS Enabled by Joe		</title>
		<link>https://aspenmesh.io/running-stateless-apps-with-service-mesh-kubernetes-cassandra-with-istio-mtls-enabled/#comment-14</link>

		<dc:creator><![CDATA[Joe]]></dc:creator>
		<pubDate>Thu, 14 Mar 2019 16:32:37 +0000</pubDate>
		<guid isPermaLink="false">https://aspenmesh.io/?p=2041#comment-14</guid>

					<description><![CDATA[In reply to &lt;a href=&quot;https://aspenmesh.io/running-stateless-apps-with-service-mesh-kubernetes-cassandra-with-istio-mtls-enabled/#comment-13&quot;&gt;Neeraj&lt;/a&gt;.

Neeraj,
Great to see you are interested in the service mesh.

For this particular architecture, we do not need a DestinationRule, as we enabled mTLS mesh wide in the meshpolicy.  If mTLS is conflicting with cassnadra‚Äôs gossip heartbeat, it would cause to pod to fail.  You likely would not be able issue a kubectl exec to the Cassandra node.  

It looks like you are successfully issuing a kubectl exec command to the cassandra-0 node.  It appears that it is the nodetool command that is having issues, stating it is unable to connect to cassandra.

One potential reason could be, there is not enough allocated memory and Cassandra is not running properly.  Using GKE  n1-standard-2 (2GB mem) nodes or larger should be sufficient.

Feel free to contact support@aspenmesh.io.  We‚Äôll dive in and take a look.]]></description>
			<content:encoded><![CDATA[<p>In reply to <a href="https://aspenmesh.io/running-stateless-apps-with-service-mesh-kubernetes-cassandra-with-istio-mtls-enabled/#comment-13">Neeraj</a>.</p>
<p>Neeraj,<br />
Great to see you are interested in the service mesh.</p>
<p>For this particular architecture, we do not need a DestinationRule, as we enabled mTLS mesh wide in the meshpolicy.  If mTLS is conflicting with cassnadra‚Äôs gossip heartbeat, it would cause to pod to fail.  You likely would not be able issue a kubectl exec to the Cassandra node.  </p>
<p>It looks like you are successfully issuing a kubectl exec command to the cassandra-0 node.  It appears that it is the nodetool command that is having issues, stating it is unable to connect to cassandra.</p>
<p>One potential reason could be, there is not enough allocated memory and Cassandra is not running properly.  Using GKE  n1-standard-2 (2GB mem) nodes or larger should be sufficient.</p>
<p>Feel free to contact <a href="mailto:support@aspenmesh.io">support@aspenmesh.io</a>.  We‚Äôll dive in and take a look.</p>
]]></content:encoded>
		
			</item>
	</channel>
</rss>
